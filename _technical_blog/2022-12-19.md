---
title: FLOPs and parameters in Neural Networks
tags:
  - Knowledge
---

![](https://www.researchgate.net/publication/356635704/figure/tbl3/AS:1096639766888449@1638470703934/Calculation-of-learnable-parameters-and-FLOPS-for-the-proposed-model.png)

## How to compare two neural networks? 
There are several metrics:
1. Number of parameters
2. FLOPs
3. GPU occupancy (`nvidia-smi`)
4. Training/inference time


Here we only talk about `FLOPs` and `parameters`.
**Note:** Floating point of operations (FLOPs) is different with floating point of per second (FLOPS). The FLOPS is the same for the same hardware device, but the FLOPs are different for different networks.

FLOPs is related to network design: Number of layers, activation layer selection, parameters, etc.

The difference between FLOPs and parameters is shown at the top figure.

Because Convolution layer can share the kernel, its parameters is far lower than the FLOPs.

**Throughput** refers to the number of examples (or tokens) that are processed within a specific period of time, e.g., “examples (or tokens) per second”.